{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%writefile Parallel.cu\n#include <stdio.h>\n#include <malloc.h>\n#include <cuda.h>\n#define M 10\n#define Nt 10\n#define dxy 0.1\n#define dt 0.01\n#define GridSize 10\n#define BlockSize 10\n#define ThreadSize M*M/(GridSize*BlockSize) \n//==============================\nfloat funct1(int i, int j) {\n  return sin(M_PI*(i*dxy + j*dxy));\n}\nfloat funct2(int i, int j) {\n  return cos(M_PI*(i*dxy + j*dxy));\n}\n//==============================\nvoid InputData(float *U, float *V, float *C) {\n  int i,j,t;\n  for (j=0; j<M; j++) {\n    for (i=0; i<M; i++) {\n      *(U + j*M + i) = funct1(i,j);\n      *(V + j*M + i) = dt*funct2(i,j) + *(U + j*M + i);\n    }\n  }\n  for (t=0; t<Nt; t++) {\n    *(C+t) = 1/2;\n  }\n}\n//===========================\n__global__ void Computing(float *A, float *B, float *C, int t){\n  float left, right, bottom, top, center1, center2;\n  int l, index, start, stop;\n  index = blockIdx.x * blockDim.x + threadIdx.x;\n  start = index*ThreadSize;\n  stop  = start + ThreadSize;\n  for (l=start; l<stop; l++) {\n      left  =  (l%M == 0)       ? 0 : *(B + l - 1);\n      right =  (l%M == M-1)    ? 0 : *(B + l + 1);\n      bottom = (l >= M*(M-1))  ? 0 : *(B + l - M);\n      top   =  (l <= M-1)       ? 0 : *(B + l + M);\n      center1 = *(B + l);\n      center2 = *(A + l);\n      *(A + l) = 2*center1 - center2 + ((dt * dt * (*(C + t)) * (*(C + t))) / (dxy * dxy)) * (left + right + bottom + top - 4*center1);\n    }\n    __syncthreads();\n}\n//===========================\nint main() {\n  float *UCPU, *VCPU, *CCPU;\n  UCPU = (float *) malloc (M*M*sizeof(float));\n  VCPU = (float *) malloc (M*M*sizeof(float));\n  CCPU = (float *) malloc (Nt*sizeof(float));\n  InputData(UCPU, VCPU, CCPU);\n\n  // Delare and Allocate Mem on GPU\n  float *UGPU, *VGPU, *CGPU;\n  cudaMalloc((void**)&UGPU ,M*M*sizeof(float));\n  cudaMalloc((void**)&VGPU ,M*M*sizeof(float));\n  cudaMalloc((void**)&CGPU ,Nt*sizeof(float));\n\n  // Copy Input from CPU to GPU\n  cudaMemcpy(UGPU,UCPU,M*M*sizeof(float),cudaMemcpyHostToDevice);\n  cudaMemcpy(VGPU,VCPU,M*M*sizeof(float),cudaMemcpyHostToDevice);\n  cudaMemcpy(CGPU,CCPU,Nt*sizeof(float),cudaMemcpyHostToDevice);\n\n  //Define Block and Thread Structure\n  dim3 dimGrid(GridSize);\n  dim3 dimBlock(BlockSize);\n\n  //Computing\n  for (int t=2; t<Nt; t++) {\n    if (t%2==0) Computing<<<dimGrid,dimBlock>>>(UGPU, VGPU, CGPU, t);\n    else Computing<<<dimGrid,dimBlock>>>(VGPU, UGPU, CGPU, t);\n  }\n\n  //Copy Output from GPU to CPU\n  cudaMemcpy(UCPU, UGPU, M*M*sizeof(float), cudaMemcpyDeviceToHost);\n  cudaMemcpy(VCPU, VGPU, M*M*sizeof(float), cudaMemcpyDeviceToHost);\n  cudaMemcpy(CCPU, CGPU, Nt*sizeof(float), cudaMemcpyDeviceToHost);\n\n  //Show results\n  if (Nt % 2 == 1) {\n    for (int j = 0; j < M; j++) {\n      for (int i = 0; i < M; i++) {\n        printf(\"%f \", *(UCPU + j * M + i));\n      }\n      printf(\"\\n\");\n    }\n  } else {\n    for (int j = 0; j < M; j++) {\n      for (int i = 0; i < M; i++) {\n        printf(\"%f \", *(VCPU + j * M + i));\n      }\n      printf(\"\\n\");\n    }\n  }\n\n  // Free Mem on CPU and GPU\n  free(UCPU); free(VCPU); free(CCPU);\n  cudaFree(UGPU); cudaFree(VGPU); cudaFree(CGPU);\n  return 0;\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!nvcc Parallel.cu -lm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!./a.out","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}